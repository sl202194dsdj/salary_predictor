{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This script pulls in salary data, builds and tests several predictive models,\n",
    "   and then makes salary predictions on test data using the best model.'''\n",
    "   \n",
    "__author__ = 'Sam M. Mfalila'\n",
    "__email__ = 'sam.mfalila@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "#to adjust figsize of plots\n",
    "plt.rcParams['figure.figsize'] = (20,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data file path and target variable\n",
    "file = 'derived_data/train_data_abt.csv'\n",
    "target = 'salary'\n",
    "test_features = pd.read_csv('unzipped_data/data/test_features.csv')\n",
    "\n",
    "#list of categorical columns for numerical encoding\n",
    "catcols = ['jobType', 'major', 'industry']\n",
    "\n",
    "class Data():\n",
    "    '''Loads data, samples training data if specified, assigns featues_df and target_df\n",
    "    '''\n",
    "    def __init__(self, file, target, sample=False, n_samples=None):\n",
    "        self.file = file\n",
    "        self. sample = sample\n",
    "        self.n_sample = n_sample\n",
    "        self.target = target\n",
    "    \n",
    "    def get_data(file, sample=False, n_samples=None):\n",
    "        '''Loads train data with option to sample'''\n",
    "        data = pd.read_csv(file)\n",
    "        if sample:\n",
    "            '''Sample train data due to resource limitation'''\n",
    "            data = data.sample(n_samples, random_state=123)\n",
    "        else:\n",
    "            data = data\n",
    "        print(data.shape, 'data loaded')\n",
    "        return data\n",
    "         \n",
    "    def get_features():\n",
    "        '''Assigns features dataframe'''\n",
    "        features_df = data.drop(target, axis = 1)\n",
    "        print(features_df.shape, 'features assigned')\n",
    "        return features_df\n",
    "        \n",
    "    def get_target():\n",
    "        '''Assigns target'''\n",
    "        target_df = data[target].values\n",
    "        print(target_df.shape, '...target rows loaded')\n",
    "        return target_df\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    '''Custum transformer to extract columns passed as arguments'''\n",
    "    \n",
    "    def __init__(self, feature_names):\n",
    "        '''Class constructor'''\n",
    "        self._feature_names = feature_names\n",
    "        \n",
    "    def fit(self, features_df, target = None):\n",
    "        '''Returns self and nothing else'''\n",
    "        return self\n",
    "    \n",
    "    def transform( self, features_df, target = None):\n",
    "        '''This method returns selected features'''\n",
    "        return features_df[ self._feature_names]    \n",
    "    \n",
    "class CategoricalFeatsAdded( BaseEstimator, TransformerMixin):\n",
    "    ''' A custom transformer that adds 'lessthanHighSchool','eduAndExp', \n",
    "        and 'notEduNotExp' features\n",
    "        Added option to delete redundant features\n",
    "    '''\n",
    "    def __init__ (self, less_than_HighSchool = False, eduAndExp = False, notEduNotExp = False):\n",
    "        ''' Class constructor'''\n",
    "        self._less_than_HighSchool = less_than_HighSchool\n",
    "        self._eduAndExp = eduAndExp\n",
    "        self._notEduNotExp = notEduNotExp\n",
    "    \n",
    "    def fit( self, features_df, target = None):\n",
    "        ''' Returns self, nothing else is done here'''\n",
    "        return self\n",
    "\n",
    "    def transform(self, features_df, target = None):\n",
    "        ''' A custom transformer that creates aformentioned features and drops redundant ones'''\n",
    "    \n",
    "        if self._less_than_HighSchool:\n",
    "            ''' Check if needed, create new column'''\n",
    "            features_df['less_than_HighSchool'] = (np.logical_and(features_df.degree == 'NONE', features_df.major == 'NONE')).astype(int)\n",
    "          \n",
    "        if self._eduAndExp:\n",
    "            '''Check if needed'''\n",
    "            features_df['eduAndexp'] = (np.logical_and(features_df.degree == 'MASTERS', features_df.yearsExperience > 5)).astype(int)\n",
    "        \n",
    "        if self._notEduNotExp:\n",
    "            '''Check if needed'''\n",
    "            features_df['notEduNotExp'] = (np.logical_and(features_df.major == 'NONE', features_df.yearsExperience < 1)).astype(int)\n",
    "            '''drop redundant columns'''\n",
    "            features_df.drop('yearsExperience', axis = 1)\n",
    "        ''' Converting any infinity values in the dataset to Nan'''\n",
    "        \n",
    "        features_df = features_df.replace( [ np.inf, -np.inf ], np.nan )\n",
    "        '''returns a numpy array'''    \n",
    "        return features_df    \n",
    "    \n",
    "class CategorizeOrdinal(BaseEstimator, TransformerMixin):\n",
    "    ''' A transformer to ordinal encode the 'degee' feature\n",
    "        This transformer we'll encode the degree feature as follows:\n",
    "        NONE:0,HIGH_SCHOOL:1, BACHELORS:2,MASTERs:3, and DOCTORAL:4 \n",
    "    '''\n",
    "    def __init__(self, features_df):\n",
    "        self.features_df = features_df\n",
    "               \n",
    "    def fit(self,features_df=None, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df=None, target=None):\n",
    "        categories = pd.Categorical(features_df.iloc[:,1],\n",
    "                                categories = ['NONE','HIGH_SCHOOL','BACHELORS','MASTERS','DOCTORAL'],\n",
    "                                ordered = True)\n",
    "        labels, unique = pd.factorize(categories, sort = True)\n",
    "        features_df.iloc[:,1] = labels\n",
    "        return features_df\n",
    "\n",
    "class CategoricalFactorize(BaseEstimator, TransformerMixin):\n",
    "    '''This class transforms categorical features using pandas pd.factorize()'''\n",
    "    def __init__(self, catcols):\n",
    "        self.catcols = catcols\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        for c in catcols:\n",
    "            encoded, categories = features_df[c].factorize()\n",
    "            #replace selected column with encoded values\n",
    "            features_df[c] = encoded\n",
    "        return features_df.values   \n",
    "\n",
    "class DelUnusedCols(BaseEstimator, TransformerMixin):\n",
    "    '''This transformer deletes unused columns from a data pipeline\n",
    "       Col 4 holds an extra column for 'yearsExperience' added through the categorical feats. pipeline.\n",
    "       This row is no longer needed after new categorical features leveraging the column are engineered\n",
    "    '''\n",
    "    def __init__(self, features_df, target=None):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        a = features_df\n",
    "        a = np.delete(a,4,1)\n",
    "        features_df = a\n",
    "        return features_df\n",
    "    \n",
    "class NumericalTransformer( BaseEstimator, TransformerMixin):\n",
    "    ''' Custom transformer we wrote to engineer  numerical features \n",
    "        Passed as boolean arguments to its constructor\n",
    "    '''\n",
    "    pass    \n",
    "\n",
    "class StandardScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    ''' This transformer standardizes all numerical features'''\n",
    "    def __init__(self, features_df, target=None):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        col_names = numerical_features\n",
    "        features = features_df[col_names]\n",
    "        scaler = StandardScaler().fit(features.values)\n",
    "        features_df = scaler.transform(features.values)\n",
    "        return features_df    \n",
    "    \n",
    "class Models(object):\n",
    "    '''This class holds all modeling objects\n",
    "       Note: Instantiate any additional models to test as class variables below    \n",
    "    '''\n",
    "    #class variables\n",
    "    estimators = {}\n",
    "    fitted_grid = {}\n",
    "    param_grids = {}\n",
    "    best_models = {}\n",
    "    best_score = None\n",
    "    best_model = []\n",
    "    best_params = {}\n",
    "    best_model_params = []\n",
    "    best_model_list = []\n",
    "    best_estimator = {}\n",
    "    rf = RandomForestRegressor(random_state=123)\n",
    "    gb = GradientBoostingRegressor(random_state = 123)\n",
    "    lasso = Lasso(random_state=123)\n",
    "    \n",
    "    def __init__(self, master, randforest,gradboost, lso,n_iter, scoring, n_jobs, train_features):\n",
    "        self.master = master\n",
    "        self.randforest= randforest\n",
    "        self.gradboost = gradboost\n",
    "        self.lso = lso\n",
    "        self.n_iter = n_iter\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.train_features   \n",
    "        \n",
    "    def hyperparameters(randforest=True, gradboost=True, lso=True):\n",
    "        '''Defines model hyperparameters for tuning\n",
    "           Add aditional models hyperparameters for tuning here as needed        \n",
    "        '''\n",
    "        if randforest:\n",
    "            '''Setting RandomForestRegressor hyperparameters for tuning'''\n",
    "            #Selecting hyperparameters for rf\n",
    "            rf_n_estimators_options =[10,75,100,150,200,1000]\n",
    "            rf_max_features_options = ['auto','sqrt', 'log2',  0.33]\n",
    "\n",
    "            #Setting param grid for rf\n",
    "            rf_param_grid = dict(n_estimators = rf_n_estimators_options, \n",
    "                     max_features = rf_max_features_options)\n",
    "            #Adding param grid for rf to param_grids\n",
    "            Models.param_grids['rf'] = rf_param_grid\n",
    "        \n",
    "        if gradboost:\n",
    "            '''Setting GradientBoostingRegressor hyperparameters for tuning'''\n",
    "            #Selecting hyperparameters for gb\n",
    "            gb_n_estimators_options = [100,200]\n",
    "            gb_learning_rate_options = [0.05,0.1,0.2]\n",
    "            gb_max_depth_options = [1,3,5]\n",
    "            subsample_options = [0.5,0.7,1.0]\n",
    "            #Setting param grid for gb\n",
    "            gb_param_grid = dict(learning_rate = gb_learning_rate_options,\n",
    "                     n_estimators = gb_n_estimators_options,\n",
    "                     subsample = subsample_options,\n",
    "                     max_depth = gb_max_depth_options)\n",
    "            Models.param_grids['gb'] = gb_param_grid\n",
    "        \n",
    "        if lso: \n",
    "            '''Setting lasso linear regression hyperparameters for tuning'''\n",
    "            #Hyperparameters for lasso\n",
    "            lasso_alpha_options = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "            #lasso param grid\n",
    "            lasso_param_grid = dict(alpha = lasso_alpha_options)\n",
    "            Models.param_grids['lasso'] = lasso_param_grid\n",
    "        print(\"Hyperparameter grid is set\")\n",
    "                                                        \n",
    "                \n",
    "    def setting_gridsearch(randforest=True, gradboost=True, lso=True):\n",
    "        '''Selects and estimators to tune and holds them in a dictionary\n",
    "           Add any additional models for fiting here as needed        \n",
    "        '''\n",
    "        #Creating a class empty dict \"estimators\" to hold estimators for GridSearchCV\n",
    "        #Add estimators to estimators dict\n",
    "        if randforest:\n",
    "            Models.estimators['rf'] = Models.rf       \n",
    "        if gradboost:\n",
    "            Models.estimators['gb'] = Models.gb            \n",
    "        if lso:\n",
    "            Models.estimators['lasso'] = Models.lasso\n",
    "        print(\"GridSearch set and ready for fitting\")\n",
    "    \n",
    "                \n",
    "    def check_hyperparams_settings():\n",
    "        '''Running code to check that hyperparameters is set up correctly.\n",
    "        '''\n",
    "        for key in ['rf', 'gb', 'lasso']:\n",
    "            if key in Models.param_grids:\n",
    "                if type(Models.param_grids[key]) is dict:\n",
    "                    print( key, 'was found in hyperparameters, and it is a grid.' )\n",
    "                else:\n",
    "                    print( key, 'was found in hyperparameters, but it is not a grid.' )\n",
    "            else:\n",
    "                print( key, 'was not found in hyperparameters')\n",
    "\n",
    "                         \n",
    "    def fit_models(n_iter, scoring, n_jobs, train_features):\n",
    "        '''Fits all models in GridSearch with k-folds cross validation\n",
    "           cv: Number of cross validation splits\n",
    "           scoring: Scoring metric\n",
    "           n_jobs: Number of processors to use if parrallel processing available(-1 means using all processors)\n",
    "        \n",
    "           Created a class dict 'fitted_grid' to hold each of fitted model\n",
    "           \n",
    "           Note that GridSearch only stores results from cross_val for the last fitted model.\n",
    "           We need to append results of each model fit to the dictionary above so we can access those attributes as needed.\n",
    "        '''\n",
    "        print('Model fitting started...')\n",
    "        for Models.name, Models.estimator in Models.estimators.items():\n",
    "            full_grid = RandomizedSearchCV(Models.estimators[Models.name], \n",
    "                                     Models.param_grids[Models.name], n_iter=n_iter, scoring = scoring,\n",
    "                                     n_jobs = n_jobs)  \n",
    "            \n",
    "            #Fit data to GridSearchCV object\n",
    "            full_grid.fit(train_features, target_df)    \n",
    "            \n",
    "            #Store fitted model\n",
    "            Models.fitted_grid[Models.name] = full_grid    \n",
    "            \n",
    "            #Print '{name} has been fitted'\n",
    "            print(Models.name,'has been fitted,')\n",
    "        \n",
    "        print('Model fitting completed.')\n",
    "                         \n",
    "    def get_best_models():\n",
    "        '''loop and iterates through Models.fitted_grids dict, \n",
    "           Selects the best model name and best scores during cross validation for each estimator group, \n",
    "           and stores the name and best score for each estimator in a dict we call \"best_models\"\n",
    "        '''\n",
    "        #Best score per model\n",
    "        for Models.name, Models.model in Models.fitted_grid.items():\n",
    "            Models.best_models[Models.name] = ( abs(Models.model.best_score_))\n",
    "        print(\"Best models and scores stored in best_models dict\")\n",
    "    \n",
    "    def get_best_score():\n",
    "        '''Here we create a variable \"best_score\" to hold the minimum mse calculated from estimator attributes strored \n",
    "           in the best_models dict above\n",
    "        '''\n",
    "        Models.best_score = min(Models.best_models.values())\n",
    "        print('\\nBest model score:', Models.best_score)\n",
    "    \n",
    "    def get_best_model_name():\n",
    "        '''Here we create a variable \"best_model\" to hold the name of the model in the best_models dict that matches\n",
    "           the minimum mse value stored in the variable best_score in part 2 above\n",
    "        '''\n",
    "        Models.best_model = [Models.key for Models.key in Models.best_models \n",
    "                             if Models.best_models[Models.key] == Models.best_score]\n",
    "        \n",
    "    def get_best_models_params():\n",
    "        '''This code loops through our dictionary holding fitted models from cross_val (\"fitted_grid\") and selects the\n",
    "           best model name and best hyparameter settings during cross_val for each estimator group and stores them on \n",
    "           best_params dict\n",
    "        '''\n",
    "        for Models.name, Models.model in Models.fitted_grid.items():\n",
    "            Models.best_params[Models.name] = (Models.model.best_params_)\n",
    "    \n",
    "    def print_best_model_params():\n",
    "        '''This code loops through the best_params dict created in step 4 above and returns paramaters\n",
    "           (i.e. the value of the key) that matches name of the model stored in the best_model list created in step 3\n",
    "        '''\n",
    "        Models.best_model_params = [(Models.key, Models.value) for Models.key, Models.value in Models.best_params.items() \n",
    "                                    if Models.key.startswith(Models.best_model[0])]   \n",
    "\n",
    "    def update_wining_hyperparams():\n",
    "        '''Updates our best model with wining hyperparameters by\n",
    "           1. Creating a dict to hold the name and hyperparameter values of our model\n",
    "           2. Using .update() to update params_grid with key, values from our dict created in 1\n",
    "        '''\n",
    "        Models.best_model_list = [(Models.model) for Models.model, Models.params in Models.best_params.items()\n",
    "                           if Models.model.startswith(Models.best_model[0])]\n",
    "\n",
    "        '''This code creates a dict to hold the name and best params of our best model\n",
    "           Creating this dict enables us to pull the key of our best model using best_model_dict[0] so we can\n",
    "           use it in our next code\n",
    "        '''        \n",
    "        Models.best_estimator[Models.best_model_list[0]] = Models.best_params[Models.best_model[0]]    \n",
    "\n",
    "        #Here we use the best_estimator dict created above to update param_grid with best hyerparameter values\n",
    "        Models.param_grids.update(Models.best_estimator)\n",
    "        print(\"Best model hyperparameters updated to hyperparameter grid\")\n",
    "        \n",
    "    def save_best_model(name):\n",
    "        '''Saves our best model as a pkl file\n",
    "           name: Given name.pkl for saving the model\n",
    "        '''\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump(Models.fitted_grid[Models.best_model[0]].best_estimator_, f)            \n",
    "                            \n",
    "class TestModels(object):\n",
    "    #Class variables\n",
    "    #Initiating the following empty dataframes as class variables\n",
    "    test_df = pd.DataFrame()\n",
    "    prepared_test_features = pd.DataFrame()\n",
    "    pred = []\n",
    "    loaded_model=[]\n",
    "    \n",
    "    ''' Holds all model testing objects'''\n",
    "    def __init__(self, fitted_grid, best_model, data, df, test_data=True):\n",
    "        self.test_data = test_data\n",
    "        self.fitted_grid = fitted_grid\n",
    "        self.best_model = best_model\n",
    "        self.data = data\n",
    "        self.n_samples = df\n",
    "    \n",
    "    def sample_test_df(n_samples):\n",
    "        '''Samples test_df created above for testing and \n",
    "           updates TestModels.prepared_features_test\n",
    "        '''\n",
    "        TestModels.prepared_features_test = TestModels.test_df.sample(n_samples, random_state = 12345)\n",
    "        print(\"prepared features for test sample\", TestModels.prepared_features_test.shape)          \n",
    "\n",
    "    def predict():\n",
    "        '''Calls ppredict on test data\n",
    "        '''\n",
    "        #TestModels.pred = Models.fitted_grid[Models.best_model[0]].predict(data)\n",
    "        TestModels.pred = TestModels.loaded_model.predict(test_features_transformed)#Use to verify loaded model works                           \n",
    "        print(\"Predictions Returned: \", TestModels.pred)\n",
    "\n",
    "    def load_saved_model(name):\n",
    "        '''Loads saved pkl file'''\n",
    "        with open(name, 'rb') as f:\n",
    "            TestModels.loaded_model = pickle.load(f)\n",
    "    \n",
    "    def save_predictions(name):\n",
    "        '''Writes predictions to csv'''\n",
    "        #Add results back to test data\n",
    "        test_features['salary'] = TestModels.pred        \n",
    "        # Remove uneeded columns and saving to a new dataframe 'predictions'\n",
    "        predictions = test_features.drop(test_features.columns[[1,2,3,4,5,6,7]], axis = 1)        \n",
    "        #Write to csv without compression\n",
    "        predictions.to_csv(name, index=False)\n",
    "        print('Predictions saved to: ', name)\n",
    "        \n",
    "    def print_summary():\n",
    "        '''Prints summary results for best model'''\n",
    "        print('\\nModel Summaries: \\n')\n",
    "        #for Models.name, Models.model in Models.fitted_grid.items():\n",
    "            #print('\\n', Models.name, 'MSE' ( abs(Models.model.best_score_))) \n",
    "        for Models.model, Models.score in Models.best_models.items():\n",
    "            print('\\n', Models.model, 'MSE:', round((Models.score),4))\n",
    "        print('\\nBest Model: ', (Models.best_model[0]))        \n",
    "        print('\\nBest Score: ', round(Models.best_score, 2))\n",
    "        \n",
    "    def get_impactful_feats(score_func, k):\n",
    "        '''Calculates and returns k best impactful features'''\n",
    "        #apply SelectKBest class to extract top k best features\n",
    "        bestfeatures = SelectKBest(score_func=score_func, k=k)\n",
    "        fit = bestfeatures.fit(np.absolute(prepared_features_df),target_df)#X values should be non-negative\n",
    "        dfscores = pd.DataFrame(fit.scores_)\n",
    "        dfcolumns = pd.DataFrame(features_df.columns)\n",
    "        #concat two dataframes for better visualization\n",
    "        TestModels.featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "        \n",
    "        TestModels.featureScores.columns = ['Feats','Score']  #naming the dataframe columns\n",
    "        TestModels.featureScores = TestModels.featureScores.sort_values('Score', ascending=False) #sorting bars on plot\n",
    "        print('\\nBest', k, 'impactful features: \\n', TestModels.featureScores.nlargest(k,'Score'))  #print k best features\n",
    "        TestModels.featureScores.plot.bar() #Plot featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 9) data loaded\n",
      "(100000, 8) features assigned\n",
      "(100000,) ...target rows loaded\n",
      "Hyperparameter grid is set\n",
      "GridSearch set and ready for fitting\n",
      "Model fitting started...\n",
      "rf has been fitted,\n",
      "gb has been fitted,\n",
      "lasso has been fitted,\n",
      "Model fitting completed.\n",
      "Best models and scores stored in best_models dict\n",
      "\n",
      "Best model score: 357.9104414221705\n",
      "Best model hyperparameters updated to hyperparameter grid\n",
      "Predictions Returned:  [122.2849517  106.46774736 132.45890849 ...  95.36498755 124.13407195\n",
      " 123.51536767]\n",
      "Predictions saved to:  test_salaries.csv\n",
      "\n",
      "Model Summaries: \n",
      "\n",
      "\n",
      " rf MSE: 426.6986\n",
      "\n",
      " gb MSE: 357.9104\n",
      "\n",
      " lasso MSE: 909.8343\n",
      "\n",
      "Best Model:  gb\n",
      "\n",
      "Best Score:  357.91\n",
      "\n",
      "Best 5 impactful features: \n",
      "        Feats         Score\n",
      "2    jobType  32423.430658\n",
      "1  companyId  17981.419343\n",
      "0      jobId  11252.331634\n",
      "3     degree    559.162598\n",
      "4      major    400.967621\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Load data\n",
    "    data = Data.get_data(file, sample=True, n_samples = 100000)\n",
    "    features_df = Data.get_features()\n",
    "    target_df = Data.get_target()\n",
    "    \n",
    "    '''Data transformation pipelines'''\n",
    "    #Categrical features to pass down categorical pipeline \n",
    "    categorical_features = ['jobType','degree', 'major', 'industry', 'yearsExperience']\n",
    "    \n",
    "    #Numerical features to pass down numerical pipeline \n",
    "    numerical_features = ['yearsExperience', 'milesFromMetropolis']\n",
    "    \n",
    "    #Define steps in categorical pipeline \n",
    "    categorical_pipeline = Pipeline( steps = [ ('cat_selector', FeatureSelector(categorical_features)),                                                                       \n",
    "                                               ('cat_feats_add', CategoricalFeatsAdded()),                                                                          \n",
    "                                               ('cat_ordinal', CategorizeOrdinal(features_df)),\n",
    "                                              \n",
    "                                               ('cat_factorize', CategoricalFactorize(catcols)),                                                                   \n",
    "                                               ('delete_unused', DelUnusedCols(features_df))\n",
    "                                            ])\n",
    "    \n",
    "    #Define steps in numerical pipeline \n",
    "    numerical_pipeline = Pipeline( steps = [ ('num_selector', FeatureSelector(numerical_features)),\n",
    "                                             ('standard_trans', StandardScalerTransformer(features_df))\n",
    "                                           ] )\n",
    "    \n",
    "    #Combine numerical and categorical pieplines horizontally using FeatureUnion\n",
    "    full_pipeline = FeatureUnion( transformer_list = [ ('categorical_pipeline', categorical_pipeline), \n",
    "                                                       ('numerical_pipeline', numerical_pipeline) ] )    \n",
    "    #Disable pandas chained_assignment warning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    #Fit data to data transformation pipeline\n",
    "    prepared_features_df = full_pipeline.fit_transform(features_df, target)\n",
    "    #print('All features_transformed successfully')\n",
    "    \n",
    "    #Set hyperparameters grid\n",
    "    Models.hyperparameters(randforest=True,gradboost=True, lso=True)\n",
    "    \n",
    "    #Set RandomForest, GradientBoosting, and Lasso regressors for fit\n",
    "    Models.setting_gridsearch(randforest=True,gradboost=True,lso=True)\n",
    "    \n",
    "    #set cross_val grid\n",
    "    Models.fit_models(n_iter=5, scoring= 'neg_mean_squared_error', n_jobs = -1, \n",
    "                      train_features = prepared_features_df )    \n",
    "    \n",
    "    #Cross validate models, then select, fit, and score test data with best model         \n",
    "    Models.get_best_models()    \n",
    "    Models.get_best_score()\n",
    "    Models.get_best_model_name()\n",
    "    Models.get_best_models_params()\n",
    "    Models.print_best_model_params()\n",
    "    Models.update_wining_hyperparams()\n",
    "                        \n",
    "    #Save/load best model\n",
    "    Models.save_best_model('final_model_salary_predictor.pkl')\n",
    "    TestModels.load_saved_model('final_model_salary_predictor.pkl')\n",
    "    \n",
    "    #Transform test features\n",
    "    #Fit data to data transformation pipeline\n",
    "    test_features_transformed = full_pipeline.fit_transform(test_features)   \n",
    "    \n",
    "    #Predict test data\n",
    "    TestModels.predict()\n",
    "    \n",
    "    #Write predictions to csv\n",
    "    TestModels.save_predictions(name='test_salaries.csv')\n",
    "    \n",
    "    #Print model summaries\n",
    "    TestModels.print_summary()\n",
    "    \n",
    "    #Get top impactful features\n",
    "    TestModels.get_impactful_feats(chi2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 9) data loaded\n"
     ]
    }
   ],
   "source": [
    "data = Data.get_data(file, sample=True, n_samples = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 8) features assigned\n"
     ]
    }
   ],
   "source": [
    "features_df = Data.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) ...target rows loaded\n"
     ]
    }
   ],
   "source": [
    "target_df = Data.get_target()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transforming Data\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"400\" src=\"Images/data_trans_pipe.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['jobType','degree', 'major', 'industry', 'yearsExperience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['yearsExperience', 'milesFromMetropolis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining steps in the categorical pipeline \n",
    "categorical_pipeline = Pipeline( steps = [ ( 'cat_selector', FeatureSelector(categorical_features)),\n",
    "                                                                                                                              \n",
    "                                           ( 'cat_feats_add', CategoricalFeatsAdded()),\n",
    "                                                                                                                         \n",
    "                                           ('cat_ordinal', CategorizeOrdinal(features_df)),\n",
    "                                          \n",
    "                                           ('cat_factorize', CategoricalFactorize(catcols)),\n",
    "                                                                                                                   \n",
    "                                           ('delete_unused', DelUnusedCols(features_df))\n",
    "                                         \n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the steps in the numerical pipeline \n",
    "numerical_pipeline = Pipeline( steps = [ ( 'num_selector', FeatureSelector(numerical_features) ),\n",
    "                                  \n",
    "                                         ('standard_trans', StandardScalerTransformer(features_df))\n",
    "                                   \n",
    "                                       ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining numerical and categorical piepline into one full big pipeline horizontally using FeatureUnion\n",
    "full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "                                                   ( 'numerical_pipeline', numerical_pipeline  ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disabling pandas chained_assignment warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our data to our data transformation pipeline\n",
    "prepared_features_df = full_pipeline.fit_transform(features_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  0.        , -1.1057991 ,\n",
       "         1.63424937],\n",
       "       [ 1.        ,  0.        ,  0.        ,  1.        ,  1.24873305,\n",
       "         0.45812425],\n",
       "       [ 2.        ,  4.        ,  1.        ,  1.        ,  0.97172927,\n",
       "         0.04302127],\n",
       "       ...,\n",
       "       [ 7.        ,  0.        ,  0.        ,  6.        ,  1.38723494,\n",
       "        -1.68657448],\n",
       "       [ 4.        ,  1.        ,  0.        ,  1.        ,  1.66423873,\n",
       "         1.39210596],\n",
       "       [ 0.        ,  2.        ,  5.        ,  3.        ,  0.27921981,\n",
       "        -1.65198257]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling Data\n",
    "\n",
    "## Setting Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter grid is set\n"
     ]
    }
   ],
   "source": [
    "#Setting hyperparameters grid\n",
    "Models.hyperparameters(randforest=True,gradboost=True, lso=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf was found in hyperparameters, and it is a grid.\n",
      "gb was found in hyperparameters, and it is a grid.\n",
      "lasso was found in hyperparameters, and it is a grid.\n"
     ]
    }
   ],
   "source": [
    "#Validating hyperparameters set correctly\n",
    "Models.check_hyperparams_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch set and ready for fitting\n"
     ]
    }
   ],
   "source": [
    "#Setting up RandomForest, GradientBoosting, and Lasso regressors for fitting\n",
    "Models.setting_gridsearch(randforest=True,gradboost=True,lso=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting started...\n",
      "rf has been fitted,\n",
      "gb has been fitted,\n",
      "lasso has been fitted,\n",
      "Model fitting completed.\n"
     ]
    }
   ],
   "source": [
    "#Fitting models that were set manually in GridSearchCV \n",
    "Models.fit_models(n_iter=5, scoring= 'neg_mean_squared_error', n_jobs = -1,\n",
    "                  train_features = prepared_features_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best models and scores stored in best_models dict\n",
      "\n",
      "Best model score: 361.9487173439982\n",
      "Best model hyperparameters updated to hyperparameter grid\n",
      "\n",
      "Model Summaries: \n",
      "\n",
      "\n",
      " rf MSE: 424.8394\n",
      "\n",
      " gb MSE: 361.9487\n",
      "\n",
      " lasso MSE: 909.8344\n",
      "\n",
      "Best Model:  gb\n",
      "\n",
      "Best Score:  361.95\n",
      "\n",
      "Best 5 impactful features: \n",
      "        Feats         Score\n",
      "2    jobType  32423.430658\n",
      "1  companyId  17981.419343\n",
      "0      jobId  11252.331634\n",
      "3     degree    559.162598\n",
      "4      major    400.967621\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model summeries\n",
    "Models.get_best_models()    \n",
    "Models.get_best_score()\n",
    "Models.get_best_model_name()\n",
    "Models.get_best_models_params()\n",
    "Models.print_best_model_params()\n",
    "Models.update_wining_hyperparams()\n",
    "TestModels.print_summary()\n",
    "TestModels.get_impactful_feats(chi2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting prepared data back to a pandas DataFrame\n",
    "test_df = pd.DataFrame(data = prepared_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n"
     ]
    }
   ],
   "source": [
    "# Sampling 5 random observations to test if our model can predict\n",
    "prepared_features_test = test_df.sample(5, random_state=12345)\n",
    "print(prepared_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sampled dataset using fitted best model\n",
    "pred = Models.fitted_grid[Models.best_model[0]].predict(prepared_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126.59756061,  58.36647686,  83.54122991, 103.5626298 ,\n",
       "       134.41206148])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.save_best_model('final_model_salary_predictor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestModels.load_saved_model('final_model_salary_predictor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           9041        8576         144         109         321         118\r\n",
      "Swap:          2047        1165         882\r\n"
     ]
    }
   ],
   "source": [
    "#Assessing available sys memory\n",
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test_features\n",
    "test_features_transformed = full_pipeline.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  0.        ,  1.38606032,\n",
       "         0.81252443],\n",
       "       [ 1.        ,  0.        ,  0.        ,  1.        ,  1.10878991,\n",
       "        -0.08745034],\n",
       "       [ 2.        ,  3.        ,  1.        ,  0.        ,  0.69288431,\n",
       "        -1.40279809],\n",
       "       ...,\n",
       "       [ 6.        ,  0.        ,  0.        ,  2.        , -1.52527889,\n",
       "         1.43558389],\n",
       "       [ 2.        ,  4.        ,  5.        ,  2.        ,  0.27697871,\n",
       "         0.46638029],\n",
       "       [ 1.        ,  0.        ,  0.        ,  2.        ,  0.55424911,\n",
       "        -0.64128097]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "test_features_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting test_features_transformed with our loaded saved pkl model\n",
    "pred_test_results = TestModels.loaded_model.predict(test_features_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118.36294886, 104.98038542, 125.65092091, ...,  95.4119308 ,\n",
       "       120.37938049, 121.55133674])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending results back to test dataframe\n",
    "\n",
    "To add predictions back to the test data frame:\n",
    "    - We append the predictions array to the original test dataframe\n",
    "    - Delete any uneeded columns\n",
    "    - Save dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features['salary'] = pred_test_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>companyId</th>\n",
       "      <th>jobType</th>\n",
       "      <th>degree</th>\n",
       "      <th>major</th>\n",
       "      <th>industry</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>milesFromMetropolis</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362685407687</td>\n",
       "      <td>COMP33</td>\n",
       "      <td>MANAGER</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>22</td>\n",
       "      <td>73</td>\n",
       "      <td>118.362949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362685407688</td>\n",
       "      <td>COMP13</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>104.980385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362685407689</td>\n",
       "      <td>COMP10</td>\n",
       "      <td>CTO</td>\n",
       "      <td>MASTERS</td>\n",
       "      <td>BIOLOGY</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>125.650921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362685407690</td>\n",
       "      <td>COMP21</td>\n",
       "      <td>MANAGER</td>\n",
       "      <td>HIGH_SCHOOL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>OIL</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>124.749204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362685407691</td>\n",
       "      <td>COMP36</td>\n",
       "      <td>JUNIOR</td>\n",
       "      <td>DOCTORAL</td>\n",
       "      <td>BIOLOGY</td>\n",
       "      <td>OIL</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>139.121754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              jobId companyId  jobType       degree    major industry  \\\n",
       "0  JOB1362685407687    COMP33  MANAGER  HIGH_SCHOOL     NONE   HEALTH   \n",
       "1  JOB1362685407688    COMP13   JUNIOR         NONE     NONE     AUTO   \n",
       "2  JOB1362685407689    COMP10      CTO      MASTERS  BIOLOGY   HEALTH   \n",
       "3  JOB1362685407690    COMP21  MANAGER  HIGH_SCHOOL     NONE      OIL   \n",
       "4  JOB1362685407691    COMP36   JUNIOR     DOCTORAL  BIOLOGY      OIL   \n",
       "\n",
       "   yearsExperience  milesFromMetropolis      salary  \n",
       "0               22                   73  118.362949  \n",
       "1               20                   47  104.980385  \n",
       "2               17                    9  125.650921  \n",
       "3               14                   96  124.749204  \n",
       "4               10                   44  139.121754  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uneeded columns and saving to a new dataframe 'predictions'\n",
    "predictions = test_features.drop(test_features.columns[[1,2,3,4,5,6,7]], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JOB1362685407687</td>\n",
       "      <td>118.362949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOB1362685407688</td>\n",
       "      <td>104.980385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOB1362685407689</td>\n",
       "      <td>125.650921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOB1362685407690</td>\n",
       "      <td>124.749204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB1362685407691</td>\n",
       "      <td>139.121754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              jobId      salary\n",
       "0  JOB1362685407687  118.362949\n",
       "1  JOB1362685407688  104.980385\n",
       "2  JOB1362685407689  125.650921\n",
       "3  JOB1362685407690  124.749204\n",
       "4  JOB1362685407691  139.121754"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To csv without compression\n",
    "predictions.to_csv('salaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> *END* </center></h1>\n",
    "<h1><center>***Thank you!***</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
